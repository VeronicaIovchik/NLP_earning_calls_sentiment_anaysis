{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25727,"status":"ok","timestamp":1677949973636,"user":{"displayName":"נועם דו","userId":"10442588050074480387"},"user_tz":-120},"id":"cQ8TSPcskhxE","outputId":"b0a20205-1629-4ddd-e96e-9bf6e8903c59"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11211,"status":"ok","timestamp":1677949984837,"user":{"displayName":"נועם דו","userId":"10442588050074480387"},"user_tz":-120},"id":"REWd8uzdk4AF","outputId":"618570e8-406d-437b-c026-74f16cffbe4b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7_OtAUf1lGJL"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","import shutil\n","import sys  \n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from bs4 import BeautifulSoup\n","import re\n","import nltk\n","from nltk.tokenize.toktok import ToktokTokenizer\n","from tqdm.notebook import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zgZCSYt3lKbo"},"outputs":[],"source":["data_final =  '/content/drive/MyDrive/Final Project/Final Data/final_data'\n","data_final = pd.read_csv(data_final)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15DTk0rK4OZg"},"outputs":[],"source":["df = data_final[['content','label']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b2vm4tpPtKx3"},"outputs":[],"source":["import json\n","with open('/content/drive/MyDrive/Final Project/Data Preprocessing/Part 1/data_matrix_3D_with_no_stopwords_after_unite_para.json', 'r') as file:\n","    data = json.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"APaPXJstcAUJ"},"outputs":[],"source":["import random\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gSlx_NKst6b7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677950017394,"user_tz":-120,"elapsed":18,"user":{"displayName":"נועם דו","userId":"10442588050074480387"}},"outputId":"c4a8abb3-7fef-4cbe-9a8e-eedb3a45e2be"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-11-0ec29629a76c>:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df['content_v1'] = data\n"]}],"source":["df['content_v1'] = data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YHxZOfrglaJH"},"outputs":[],"source":["target_list = ['0', '1', '2']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wIY2znP3m9re"},"outputs":[],"source":["# Create new columns for each category and assign values\n","df['0'] = df['label'].apply(lambda x: 1 if x == 0 else 0)\n","df['1'] = df['label'].apply(lambda x: 1 if x == 1 else 0)\n","df['2'] = df['label'].apply(lambda x: 1 if x == 2 else 0)\n","\n","# Remove the original column\n","df.drop('label', axis=1, inplace=True)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vO8PYl6tur34"},"outputs":[],"source":["df.drop('content',axis=1,inplace=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5LNhfzM1vGnp"},"outputs":[],"source":["df = df.rename(columns={'content_v1':'content'})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mROh7ZsOlkuL"},"outputs":[],"source":["# hyperparameters\n","MAX_LEN = 450\n","TRAIN_BATCH_SIZE = 64\n","VALID_BATCH_SIZE = 64\n","EPOCHS = 2\n","LEARNING_RATE = 1e-04"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VIRFK5BElnC9"},"outputs":[],"source":["from transformers import BertTokenizer, Trainer, BertForSequenceClassification, TrainingArguments\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SF0Ktc8wKm9w"},"outputs":[],"source":["model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-pretrain',num_labels=3)\n","tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-pretrain')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1UvRkSezNmxQ"},"outputs":[],"source":["# Split to train,validation,test\n","from sklearn.model_selection import train_test_split\n","\n","train_df, val_df = train_test_split(df ,test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gFlZAu3SPf_-"},"outputs":[],"source":["train_df, test_df =train_test_split(train_df ,test_size=0.25, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IRwmaTFXSuxC"},"outputs":[],"source":["train_df = train_df.reset_index(drop=False)\n","train_df = train_df.sort_values('index')\n","train_df = train_df.drop('index',axis=1).reset_index(drop=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YQD7irYxV6Uo"},"outputs":[],"source":["val_df = val_df.reset_index(drop=False)\n","val_df = val_df.sort_values('index')\n","val_df = val_df.drop('index',axis=1).reset_index(drop=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"APSFoy6kRM1Z"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4xPfNqe9770z"},"outputs":[],"source":["# This function takes from each sample x (x resmble the size of the batch in the model) paragraph of 450 length\n","def shuffle(lst,batch_size):\n","\n","    return lst[:batch_size]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LQK9fN448Yb6"},"outputs":[],"source":["# This function got df and return new df which contain for each sample X ( as many we decide the batch size is) sub sample of their subtext for choose randomly, for example the 32 first rows in the new df will contain the 32 subtext of the\n","# first sample and etc..\n","def smaller(df,batch_size):\n","    # Iterate over each row in the original DataFrame\n","    content_lst = []\n","    lst0 = []\n","    lst1 = []\n","    lst2 =[]\n","    for index, row in df.iterrows():\n","      temp = shuffle(row['content'],batch_size)\n","      if(len(temp)) < batch_size:\n","        continue\n","      else:\n","        for i in range(0,len(temp)):\n","          content_lst.append(temp[i])\n","          lst0.append(row['0'])\n","          lst1.append(row['1'])\n","          lst2.append(row['2'])\n","    new_df = np.array([content_lst,lst0,lst1,lst2])\n","    new_df = pd.DataFrame(new_df.T,columns = ['content','0','1','2'])\n","    return new_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pT5fqdkK_lJj"},"outputs":[],"source":["# Here we taking each sample and divide the sample to 64 parts\n","train_df = smaller(train_df,64)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"klrPbLLCJUg-"},"outputs":[],"source":["val_df = smaller(val_df,64)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vV0ZHSZtPbGr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677950066439,"user_tz":-120,"elapsed":28,"user":{"displayName":"נועם דו","userId":"10442588050074480387"}},"outputId":"1a4debc1-95d3-46c1-dc36-364efdfcaf0d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(241856, 4)"]},"metadata":{},"execution_count":27}],"source":["val_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ooiqNZcLeZ_"},"outputs":[],"source":["train_df['0'] = train_df['0'].astype('int64')\n","train_df['1'] = train_df['1'].astype('int64')\n","train_df['2'] = train_df['2'].astype('int64')\n","val_df['0'] = val_df['0'].astype('int64')\n","val_df['1'] = val_df['1'].astype('int64')\n","val_df['2'] = val_df['2'].astype('int64')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6IY_i3Y-lyf-"},"outputs":[],"source":["# This is custom dataset, for loding throughout pythorch loadr\n","# This data set return for each sample a dictinary which include his tokenization using the tokenization of finbert\n","class CustomDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, df, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.df = df\n","        self.title = df['content']\n","        self.targets = self.df[target_list].values\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.title)\n","\n","    def __getitem__(self, index):\n","        title = str(self.title[index])\n","        title = \" \".join(title.split())\n","\n","        inputs = self.tokenizer.encode_plus(\n","            title,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            padding='max_length',\n","            return_token_type_ids=True,\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': inputs['input_ids'].flatten(),\n","            'attention_mask': inputs['attention_mask'].flatten(),\n","            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n","            'targets': torch.FloatTensor(self.targets[index])\n","        }\n","          "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WIbXh0-dmpso"},"outputs":[],"source":["train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n","valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BGUQ1ydgmurH"},"outputs":[],"source":["train_data_loader = torch.utils.data.DataLoader(train_dataset, \n","    batch_size=TRAIN_BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=0\n",")\n","\n","val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n","    batch_size=VALID_BATCH_SIZE,\n","    shuffle=False,\n","    num_workers=0\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6MOB4vbyokMS"},"outputs":[],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hxoEeqLUovsI"},"outputs":[],"source":["def load_ckp(checkpoint_fpath, model, optimizer):\n","    \"\"\"\n","    checkpoint_path: path to save checkpoint\n","    model: model that we want to load checkpoint parameters into       \n","    optimizer: optimizer we defined in previous training\n","    \"\"\"\n","    # load check point\n","    checkpoint = torch.load(checkpoint_fpath)\n","    # initialize state_dict from checkpoint to model\n","    model.load_state_dict(checkpoint['state_dict'])\n","    # initialize optimizer from checkpoint to optimizer\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    # initialize valid_loss_min from checkpoint to valid_loss_min\n","    valid_loss_min = checkpoint['valid_loss_min']\n","    # return model, optimizer, epoch value, min validation loss \n","    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()\n","\n","def save_ckp(state, is_best, checkpoint_path, best_model_path):\n","    \"\"\"\n","    state: checkpoint we want to save\n","    is_best: is this the best checkpoint; min validation loss\n","    checkpoint_path: path to save checkpoint\n","    best_model_path: path to save best model\n","    \"\"\"\n","    f_path = checkpoint_path\n","    # save checkpoint data to the path given, checkpoint_path\n","    torch.save(state, f_path)\n","    # if it is a best model, min validation loss\n","    if is_best:\n","        best_fpath = best_model_path\n","        # copy that checkpoint file to best path given, best_model_path\n","        shutil.copyfile(f_path, best_fpath)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SmzA4QaBo5-9"},"outputs":[],"source":["# Building the model\n","# The model include the finbert model pre traind\n","# 3 convultional layers\n","# 2 FF layers\n","class FinBertSequenceClassifier(torch.nn.Module):\n","    def __init__(self, num_labels=3):\n","        super(FinBertSequenceClassifier, self).__init__()\n","        self.device = device\n","        self.bert_model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-pretrain',num_labels=3)\n","        self.conv1 = nn.Conv1d(in_channels=64, out_channels=256, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n","        self.dropout = nn.Dropout(0.3)\n","        self.fc = nn.Linear(3, 128)\n","        self.conv3 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, padding=1)\n","        self.fc1 = nn.Linear(128,3)\n","        self.weight_decay = 0.02\n","    \n","    def forward(self, input_ids, attn_mask, token_type_ids):\n","        output = self.bert_model(\n","            input_ids, \n","            attention_mask=attn_mask, \n","            token_type_ids=token_type_ids\n","        ).logits\n","        x = self.conv1(output)\n","        x = nn.functional.relu(x)\n","        x = self.dropout(x)\n","        x = self.conv2(x)\n","        x = nn.functional.relu(x)\n","        x = self.conv3(x)\n","        x = nn.functional.relu(x)\n","        x = self.dropout(x)\n","        x = self.fc(x)\n","        x = nn.functional.relu(x)\n","        x = self.dropout(x)\n","        logits = self.fc1(x)\n","        output = nn.functional.softmax(logits, dim=1)\n","        return output\n","\n","    \n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"idw3L9P5r2nL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677950074396,"user_tz":-120,"elapsed":7970,"user":{"displayName":"נועם דו","userId":"10442588050074480387"}},"outputId":"58eda92e-70a1-424e-e33f-d310f658a6fd"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["FinBertSequenceClassifier(\n","  (bert_model): BertForSequenceClassification(\n","    (bert): BertModel(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(30873, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Linear(in_features=768, out_features=3, bias=True)\n","  )\n","  (conv1): Conv1d(64, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n","  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv2): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (fc): Linear(in_features=3, out_features=128, bias=True)\n","  (conv3): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n","  (fc1): Linear(in_features=128, out_features=3, bias=True)\n",")"]},"metadata":{},"execution_count":35}],"source":["model = FinBertSequenceClassifier()\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4OBxBgJIE9Lm"},"outputs":[],"source":["import os\n","\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XG9VEkj2BIbA"},"outputs":[],"source":["def l2_regularization(model, weight_decay):\n","    l2_reg = torch.tensor(0.0).to(model.device)\n","    for name, param in model.named_parameters():\n","        if 'bias' not in name:\n","            l2_reg += torch.norm(param, p=2)\n","    return l2_reg * weight_decay"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XNkVw1quo_ht"},"outputs":[],"source":["def loss_function(outputs, targets, model, weight_decay):\n","    loss = nn.CrossEntropyLoss()(outputs, targets)\n","    loss += l2_regularization(model, weight_decay)\n","    return loss\n","\n","optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"imwH5LxgKhPA"},"outputs":[],"source":["# this function take all the batches output and return the average output of all the batch\n","# we do this because each batch resamble one sample (each origanl sample divided to 64 parts)\n","def get_out_final_output(outputs):\n","\n","  sum = outputs.sum(dim=0) / len(outputs)\n","  return sum\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BuARFiqmhlVQ"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_XaCKTqpJ4P"},"outputs":[],"source":["def train_model(n_epochs, training_loader, validation_loader, model, \n","                optimizer, checkpoint_path, best_model_path,device):\n","   \n","  # initialize tracker for minimum validation loss\n","  valid_loss_min = np.Inf\n","   \n"," \n","  for epoch in range(1, n_epochs+1):\n","    train_loss = 0\n","    valid_loss = 0\n","    val_correct = 0\n","    val_total = 0\n","    model.train()\n","    print('############# Epoch {}: Training Start   #############'.format(epoch))\n","    for batch_idx, data in enumerate(training_loader):\n","        ids = data['input_ids'].to(device, dtype = torch.long)\n","        mask = data['attention_mask'].to(device, dtype = torch.long)\n","        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","        targets = data['targets'].to(device, dtype = torch.float)\n","        outputs = model(ids, mask, token_type_ids)\n","        outputs =  get_out_final_output(outputs)\n","        optimizer.zero_grad()\n","        outputs = get_out_final_output(outputs)\n","        targets = targets[0]\n","        loss = loss_function(outputs, targets,model,0.01)\n","        if batch_idx%2500==0:\n","           print(f'Epoch: {epoch}, Training Loss:  {loss.item()}', 'Batch idx', {batch_idx})\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        #print('before loss data in training', loss.item(), train_loss)\n","        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n","        #print('after loss data in training', loss.item(), train_loss)\n","    \n","    print('############# Epoch {}: Training End     #############'.format(epoch))\n","    \n","    print('############# Epoch {}: Validation Start   #############'.format(epoch))\n","    ######################    \n","    # validate the model #\n","    ######################\n"," \n","    model.eval()\n","   \n","    with torch.no_grad():\n","      val_targets1 = []\n","      val_outputs1 = []\n","      for batch_idx, data in enumerate(validation_loader, 0):\n","            ids = data['input_ids'].to(device, dtype = torch.long)\n","            mask = data['attention_mask'].to(device, dtype = torch.long)\n","            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","            targets = data['targets'].to(device, dtype = torch.float)\n","            outputs = model(ids, mask, token_type_ids)\n","            outputs =  get_out_final_output(outputs)\n","            targets = targets[0]\n","            loss = loss_function(outputs, targets,model,0.01)\n","            temp = outputs.tolist()\n","            max = np.argmax(temp)\n","            new_lst = np.array([1 if i == max else 0 for i in range(len(temp))])\n","            val_outputs1.append(new_lst)\n","            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n","            val_targets1.append(np.array(targets.tolist()))\n","\n","\n","      print('############# Epoch {}: Validation End     #############'.format(epoch))\n","      # calculate average losses\n","      #print('before cal avg train loss', train_loss)\n","      train_loss = train_loss/len(training_loader)\n","      valid_loss = valid_loss/len(validation_loader)\n","\n","            # caculate accuracy\n","      val_acc = accuracy_score(val_targets1,val_outputs1)\n","      # print training/validation statistics \n","      print(\"Epoch: {}, Training Loss: {:.6f}, Validation Loss: {:.6f}, Validation Accuracy: {:.6f}\".format(\n","\n","            epoch, \n","            train_loss,\n","            valid_loss,\n","            val_acc\n","            ))\n","      \n","      # create checkpoint variable and add important data\n","      checkpoint = {\n","            'epoch': epoch + 1,\n","            'valid_loss_min': valid_loss,\n","            'state_dict': model.state_dict(),\n","            'optimizer': optimizer.state_dict()\n","      }\n","        \n","        # save checkpoint\n","      save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n","        \n","      ## TODO: save the model if validation loss has decreased\n","      if valid_loss <= valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n","        # save checkpoint as best model\n","        save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n","        valid_loss_min = valid_loss\n","\n","    print('############# Epoch {}  Done   #############\\n'.format(epoch))\n","\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vY99HvMnpPN5"},"outputs":[],"source":["ckpt_path = \"/content/drive/MyDrive/Final Project/FinBert Model/finbert\"\n","best_model_path = \"/content/drive/MyDrive/Final Project/FinBert Model/finbert_best.pt\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8nHayiJipb0f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677984045686,"user_tz":-120,"elapsed":33971321,"user":{"displayName":"נועם דו","userId":"10442588050074480387"}},"outputId":"29f9c93d-23db-4dbb-9c3c-dd4e0ae3b938"},"outputs":[{"output_type":"stream","name":"stdout","text":["############# Epoch 1: Training Start   #############\n","Epoch: 1, Training Loss:  30.893798828125 Batch idx {0}\n","Epoch: 1, Training Loss:  5.993905544281006 Batch idx {2500}\n","Epoch: 1, Training Loss:  4.1845574378967285 Batch idx {5000}\n","Epoch: 1, Training Loss:  2.4319796562194824 Batch idx {7500}\n","Epoch: 1, Training Loss:  0.8534832000732422 Batch idx {10000}\n","############# Epoch 1: Training End     #############\n","############# Epoch 1: Validation Start   #############\n","############# Epoch 1: Validation End     #############\n","Epoch: 1, Training Loss: 0.000387, Validation Loss: 0.000207, Validation Accuracy: 0.410426\n","Validation loss decreased (inf --> 0.000207).  Saving model ...\n","############# Epoch 1  Done   #############\n","\n","############# Epoch 2: Training Start   #############\n","Epoch: 2, Training Loss:  0.7691027522087097 Batch idx {0}\n","Epoch: 2, Training Loss:  0.7246585488319397 Batch idx {2500}\n","Epoch: 2, Training Loss:  0.7572054266929626 Batch idx {5000}\n","Epoch: 2, Training Loss:  0.8659952878952026 Batch idx {7500}\n","Epoch: 2, Training Loss:  0.7615646719932556 Batch idx {10000}\n","############# Epoch 2: Training End     #############\n","############# Epoch 2: Validation Start   #############\n","############# Epoch 2: Validation End     #############\n","Epoch: 2, Training Loss: 0.000068, Validation Loss: 0.000200, Validation Accuracy: 0.410426\n","Validation loss decreased (0.000207 --> 0.000200).  Saving model ...\n","############# Epoch 2  Done   #############\n","\n"]}],"source":["trained_model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path,device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7trtpEWz5q0J"},"outputs":[],"source":["test_df = test_df.reset_index(drop=False)\n","test_df = test_df.sort_values('index')\n","test_df = test_df.drop('index',axis=1).reset_index(drop=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_s4oVQ0L908p"},"outputs":[],"source":["# Here we taking each sample and divide the sample to 64 parts\n","\n","test_df = smaller(test_df,64)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6I16NhMMBNTI"},"outputs":[],"source":["test_df['0'] = test_df['0'].astype('int64')\n","test_df['1'] = test_df['1'].astype('int64')\n","test_df['2'] = test_df['2'].astype('int64')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WvT3J4JI-WCp"},"outputs":[],"source":["test_dataset = CustomDataset(test_df,tokenizer,MAX_LEN)\n","batch_size = 64"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4bdx0xG4AgGw"},"outputs":[],"source":["test_data_loader = torch.utils.data.DataLoader(test_dataset, \n","    batch_size=batch_size,\n","    shuffle=False,\n","    num_workers=0\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yh6aIXVHawMq"},"outputs":[],"source":["# this fuction calculate the accuracy of the test\n","# we divided each sample of the test to 64 parts and put them as a batch in the model\n","# each batch we calcualte the average output for example for 2 softmax outputs [0.2,0.1,0.7] & [0.8,0.1,0.1] the average output will be [0.5,0.1,0.4]\n","# because each output in the batch have the same label because their are part of the original sample we wiil take the label of the first sub sample in the batch\n","output_lst = []\n","target_lst =[]\n","model.eval()\n","with torch.no_grad():\n","    for batch_idx, data in enumerate(test_data_loader, 0):\n","      ids = data['input_ids'].to(device, dtype = torch.long)\n","      mask = data['attention_mask'].to(device, dtype = torch.long)\n","      token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","      targets = data['targets'].to(device, dtype = torch.float)\n","      outputs = model(ids, mask, token_type_ids)\n","      outputs =  get_out_final_output(outputs)\n","      temp = outputs.tolist()\n","      max = np.argmax(temp)\n","      new_lst = np.array([1 if i == max else 0 for i in range(len(temp))])\n","      output_lst.append(new_lst)\n","      targets = targets[0]\n","      target_lst.append(np.array(targets.tolist()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uym1ZpSYFbT3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677986049498,"user_tz":-120,"elapsed":65,"user":{"displayName":"נועם דו","userId":"10442588050074480387"}},"outputId":"4092e3cb-f17b-435e-a0fa-5d8b9131c52e"},"outputs":[{"output_type":"stream","name":"stdout","text":["The  Accuracy is : 0.4251258945136496\n"]}],"source":["test_acc = accuracy_score(target_lst,output_lst)\n","print('The  Accuracy is :', test_acc)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1y4Y7L44Ye9i6Qv5JZyfT9V5AYqwVqEQP","timestamp":1677854512743}]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}